# ğŸ¬  â€œMini Casting Pulseâ€ Coding Exercise

Actors find work through an online casting platformâ€”basically a job board dedicated to film, TV, commercials, and voice work. When an actor refreshes their casting feed, a live stream of brand-new breakdowns appears. A breakdown is a structured role notice: it lists the project type (film, series, commercial, etc.), shoot location, pay rate, union status, and the age/gender/ethnicity the casting director is looking for, along with a short role description.

Every time one of our users sees a role notice, we record that breakdown as a single row in a raw data table. Your challenge is to take those raw rows and roll them up into a clean, daily â€œCasting Pulseâ€ summary. This file will let industry analysts quickly spot trends such as rising day-rates, regional production shifts, or surges in particular project typesâ€”without ever exposing details about individual actors or roles.


> **TL;DR** â€“ From one raw CSV youâ€™ll generate a single `daily_pulse.csv` that buckets, aggregates, and lightly enriches the data; weâ€™ll inspect the output and your code.

---

## ğŸ“‚  Repo layout

```

data/
breakdowns_sample.csv   # \1000 breakdown rows (same schema we use in prod)
pulse.py                # <-- you create this script
daily_pulse.csv         # <-- put your final daily_pulse.csv here
README.md               # (this file)

````

---

## ğŸ¯  What you need to build

Run:

```bash
python build_pulse.py \
       --input  breakdowns_sample.csv \
       --output daily_pulse.csv
````

Your script should finish in **< 5 min** on a laptop (8 GB RAM).

### ğŸ”‘  Required columns & rules

| Column                | Build rule                                                                               |
| --------------------- | ---------------------------------------------------------------------------------------- |
| `date_utc`            | `posted_date` truncated to `YYYY-MM-DD` (UTC)                                            |
| `region_code`         | Map `work_city` â†’ `LA`, `NY`  <br>*â†³ Pick any sane mapping & document it* |
| `proj_type_code`      | Map `project_type` â†’ `F` (film), `T` (TV/streaming), `C` (commercial), `V` (voice/other) |
| `role_count_day`      | Number of rows in that bucket                                                            |
| `lead_share_pct_day`  | `(Lead + Principal rows) Ã· role_count_day`, **1 dp**                                     |
| `union_share_pct_day` | `(union rows) Ã· role_count_day`, **1 dp**                                                |
| `median_rate_day_usd` | Median `rate_value`, **rounded to the nearest \$250**                                    |
| `sentiment_avg_day`   | Score each `role_description` âˆ’1â€¦+1 (any open model), store bucket mean **rounded 0.05** |

*Extra credit:* `theme_ai_share_pct_day` â€“ % of rows whose text contains â€œAIâ€ / â€œrobotâ€ / â€œandroidâ€.

Sort by `date_utc, region_code, proj_type_code`.

---

## ğŸ¤«  Intentional vagueness â€” and what weâ€™re testing

| We left this vagueâ€¦                      | Because we want to seeâ€¦                                                                   |
| ---------------------------------------- | ----------------------------------------------------------------------------------------- |
| **Countryâ†’Region map**                   | Can you design & document a simple taxonomy?                                              |
| **Project-type mapping**                 | Can you normalise messy categorical data?                                                 |
| **Sentiment tool choice**                | Pragmatic ML instinct (TextBlob? HuggingFace? OpenAI API ? Your call)                                  |
| **Privacy touches**<br>(k-anon, Laplace) | Bonus points if you mention or add a small noise / bucket filter; not mandatory for pass. |
| **No tests, no CI**                      | Code clarity & runnable script matter more than boilerplate.                              |

---

## ğŸ“  Submission checklist

1. âœ… `build_pulse.py` â€“ clear functions, docstrings, sensible libs.
2. âœ… `output/daily_pulse.csv` â€“ â‰¤ 30 MB (gzip if larger).
3. âœ… **< 150-word** note in this README (bottom) explaining:

   * your country/region mapping
   * your project-type mapping
   * sentiment library you used
4. Push to your fork or send us a zipped repo link.

---

## ğŸš€  Hints

* **Pandas groupby** is plenty for 50 K rows.
* Median rounding: `int(round(x / 250.0)) * 250`.
* For a quick sentiment baseline: `pip install textblob`.
* Want bonus anonymisation?  Drop buckets `< 50` rows or add `np.random.laplace(0, 1)` to counts.

---

## ğŸ—’ï¸  Your short explanation here

*(replace this bullet list with your mappings & choices)*

```

---
